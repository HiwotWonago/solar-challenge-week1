{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Hiwi\\\\Documents\\\\week5\\\\data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee416de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numerical_cols].hist(bins=30, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f118d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    sns.countplot(y=col, data=df)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_percent = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_percent})\n",
    "missing_df[missing_df['Missing Count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483af904",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir('C:/Users/Hiwi/Documents/week5/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git add .\n",
    "#!git commit -m \"Initial commit: EDA and feature engineering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git remote add origin https://github.com/HiwotWonago/credit-scoring-system.git\n",
    "#!git branch -M main\n",
    "#!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ff535",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a802894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xverse.transformer import MonotonicBinning  # pip install xverse\n",
    "\n",
    "# 1. Custom Transformer for Aggregation\n",
    "class CustomerAggregator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.aggregation_dict = {\n",
    "            'Amount': ['sum', 'mean', 'count', 'std', 'min', 'max'],\n",
    "            'Value': ['sum', 'mean', 'std'],\n",
    "            'FraudResult': 'mean'  # Proxy risk score\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Extract datetime features\n",
    "        X['TransactionStartTime'] = pd.to_datetime(X['TransactionStartTime'])\n",
    "        X['Hour'] = X['TransactionStartTime'].dt.hour\n",
    "        X['Day'] = X['TransactionStartTime'].dt.day\n",
    "        X['Month'] = X['TransactionStartTime'].dt.month\n",
    "        X['Year'] = X['TransactionStartTime'].dt.year\n",
    "        \n",
    "        # Add behavioral features\n",
    "        X['is_refund'] = (X['Amount'] < 0).astype(int)\n",
    "        X['is_night'] = X['Hour'].between(0, 6).astype(int)\n",
    "        \n",
    "        # Customer-level aggregation\n",
    "        customer_df = X.groupby('CustomerId').agg(\n",
    "            total_amount=('Amount', 'sum'),\n",
    "            avg_amount=('Amount', 'mean'),\n",
    "            transaction_count=('Amount', 'count'),\n",
    "            amount_std=('Amount', 'std'),\n",
    "            refund_rate=('is_refund', 'mean'),\n",
    "            night_transaction_ratio=('is_night', 'mean'),\n",
    "            preferred_category=('ProductCategory', lambda x: x.mode()[0]),\n",
    "            preferred_channel=('ChannelId', lambda x: x.mode()[0])\n",
    "        ).reset_index()\n",
    "        \n",
    "        return customer_df\n",
    "\n",
    "# 2. Custom Transformer for Feature Extraction\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Create risk segments based on transaction patterns\n",
    "        X['amount_risk'] = pd.cut(X['avg_amount'], \n",
    "                                 bins=[-np.inf, 50, 200, 500, np.inf],\n",
    "                                 labels=['low', 'medium', 'high', 'very_high'])\n",
    "        \n",
    "        # Create customer activity segments\n",
    "        X['activity_level'] = pd.cut(X['transaction_count'],\n",
    "                                    bins=[0, 5, 20, 100, np.inf],\n",
    "                                    labels=['inactive', 'casual', 'active', 'hyperactive'])\n",
    "        return X\n",
    "\n",
    "# 3. Build Complete Pipeline\n",
    "def build_feature_pipeline():\n",
    "    # Define column types\n",
    "    numerical_features = ['total_amount', 'avg_amount', 'transaction_count', \n",
    "                         'amount_std', 'refund_rate', 'night_transaction_ratio']\n",
    "    categorical_features = ['preferred_category', 'preferred_channel',\n",
    "                           'amount_risk', 'activity_level']\n",
    "    \n",
    "    # Preprocessing transformers\n",
    "    num_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    cat_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, numerical_features),\n",
    "            ('cat', cat_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Full pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('aggregator', CustomerAggregator()),\n",
    "        ('extractor', FeatureExtractor()),\n",
    "        ('woe_binner', WOETransformer()),  # Weight of Evidence transformation\n",
    "        ('monotonic_binner', MonotonicBinning()),  # Monotonic binning from xverse\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# 4. Usage Example (In your training script)\n",
    "# from src.feature_engineering import build_feature_pipeline\n",
    "# feature_pipeline = build_feature_pipeline()\n",
    "# X_processed = feature_pipeline.fit_transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AggregateTransactionFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, id_col='CustomerId', amount_col='Amount'):\n",
    "        self.id_col = id_col\n",
    "        self.amount_col = amount_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        agg = X.groupby(self.id_col)[self.amount_col].agg(\n",
    "            total_amount='sum',\n",
    "            avg_amount='mean',\n",
    "            transaction_count='count',\n",
    "            std_amount='std'\n",
    "        ).reset_index()\n",
    "        return agg\n",
    "\n",
    "class TransactionTimeFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, datetime_col='TransactionStartTime'):\n",
    "        self.datetime_col = datetime_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.datetime_col] = pd.to_datetime(X[self.datetime_col])\n",
    "        X['transaction_hour'] = X[self.datetime_col].dt.hour\n",
    "        X['transaction_day'] = X[self.datetime_col].dt.day\n",
    "        X['transaction_month'] = X[self.datetime_col].dt.month\n",
    "        X['transaction_year'] = X[self.datetime_col].dt.year\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/pipelines.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from feature_engineering import TransactionTimeFeatures\n",
    "\n",
    "# ================================\n",
    "# Define column groups\n",
    "# ================================\n",
    "\n",
    "CATEGORICAL_COLS = [\n",
    "    'CurrencyCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'PricingStrategy'\n",
    "]\n",
    "\n",
    "NUMERICAL_COLS = [\n",
    "    'Amount', 'Value'\n",
    "]\n",
    "\n",
    "DATETIME_COL = 'TransactionStartTime'\n",
    "\n",
    "# ================================\n",
    "# Pipelines for sub-transforms\n",
    "# ================================\n",
    "\n",
    "# Pipeline for categorical features\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# ================================\n",
    "# Preprocessing ColumnTransformer\n",
    "# ================================\n",
    "\n",
    "def build_transaction_pipeline():\n",
    "    preprocessing = ColumnTransformer(transformers=[\n",
    "        ('num', numerical_pipeline, NUMERICAL_COLS),\n",
    "        ('cat', categorical_pipeline, CATEGORICAL_COLS)\n",
    "    ])\n",
    "\n",
    "    # Final pipeline with datetime features first\n",
    "    full_pipeline = Pipeline(steps=[\n",
    "        ('datetime_features', TransactionTimeFeatures(datetime_col=DATETIME_COL)),\n",
    "        ('preprocessing', preprocessing)\n",
    "    ])\n",
    "\n",
    "    return full_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb1b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
